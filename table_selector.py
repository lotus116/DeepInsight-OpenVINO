"""
æ™ºèƒ½è¡¨é€‰æ‹©ç®—æ³• - åŸºäºOpenVINOä¼˜åŒ–çš„è¯­ä¹‰åŒ¹é…
åŠ¨æ€è¯»å–ç”¨æˆ·ä¸Šä¼ çš„schemaæ–‡ä»¶ï¼Œä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œæ™ºèƒ½è¡¨é€‰æ‹©
"""

import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import os
import time
import psutil


@dataclass
class TableRelevance:
    """è¡¨ç›¸å…³æ€§è¯„åˆ†ç»“æœ"""
    table_name: str
    table_description: str
    relevance_score: float
    semantic_similarity: float  # è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†
    keyword_matches: List[str]
    matched_columns: List[Dict]
    reasoning: str
    is_primary: bool = False  # æ˜¯å¦ä¸ºä¸»è¦è¡¨
    is_join_required: bool = False  # æ˜¯å¦éœ€è¦å…³è”


class IntelligentTableSelector:
    """åŸºäºOpenVINOçš„æ™ºèƒ½è¡¨é€‰æ‹©å™¨"""
    
    def __init__(self, rag_engine=None, schema_paths=None):
        """
        åˆå§‹åŒ–è¡¨é€‰æ‹©å™¨
        :param rag_engine: å·²åˆå§‹åŒ–çš„IntelRAGå®ä¾‹ï¼Œç”¨äºå‘é‡è®¡ç®—
        :param schema_paths: ç”¨æˆ·ä¸Šä¼ çš„schemaæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        self.rag_engine = rag_engine
        self.schema_paths = schema_paths or []
        self.tables = []
        self.table_embeddings = None
        self.column_embeddings = None
        
        # åŠ¨æ€åŠ è½½schema
        self.load_dynamic_schema()
        
        # å¦‚æœæœ‰RAGå¼•æ“ï¼Œé¢„è®¡ç®—è¡¨å’Œåˆ—çš„å‘é‡
        if self.rag_engine and self.rag_engine.model:
            self.precompute_embeddings()
    
    def load_dynamic_schema(self):
        """åŠ¨æ€åŠ è½½ç”¨æˆ·ä¸Šä¼ çš„schemaæ–‡ä»¶"""
        self.tables = []
        
        if not self.schema_paths:
            print("âš ï¸ [TableSelector] æœªé…ç½®schemaæ–‡ä»¶è·¯å¾„")
            return
        
        for schema_path in self.schema_paths:
            if not os.path.exists(schema_path):
                print(f"âš ï¸ [TableSelector] Schemaæ–‡ä»¶ä¸å­˜åœ¨: {schema_path}")
                continue
                
            try:
                with open(schema_path, 'r', encoding='utf-8') as f:
                    schema_data = json.load(f)
                    
                # æ”¯æŒä¸åŒçš„schemaæ ¼å¼
                if isinstance(schema_data, list):
                    # æ ¼å¼1: ç›´æ¥æ˜¯è¡¨åˆ—è¡¨ (å¦‚northwindæ ¼å¼)
                    self.tables.extend(schema_data)
                elif isinstance(schema_data, dict):
                    # æ ¼å¼2: åŒ…è£…åœ¨å¯¹è±¡ä¸­
                    if 'tables' in schema_data:
                        self.tables.extend(schema_data['tables'])
                    elif 'schema' in schema_data:
                        self.tables.extend(schema_data['schema'])
                    else:
                        # æ ¼å¼3: å•ä¸ªè¡¨å®šä¹‰
                        self.tables.append(schema_data)
                        
                print(f"âœ… [TableSelector] æˆåŠŸåŠ è½½schema: {schema_path} ({len(schema_data)} ä¸ªè¡¨)")
                
            except Exception as e:
                print(f"âŒ [TableSelector] åŠ è½½schemaå¤±è´¥ {schema_path}: {e}")
        
        print(f"ğŸ“Š [TableSelector] æ€»å…±åŠ è½½ {len(self.tables)} ä¸ªæ•°æ®è¡¨")
    
    def precompute_embeddings(self):
        """é¢„è®¡ç®—æ‰€æœ‰è¡¨å’Œåˆ—çš„å‘é‡è¡¨ç¤º"""
        if not self.tables or not self.rag_engine:
            return
            
        print("ğŸš€ [TableSelector] æ­£åœ¨é¢„è®¡ç®—è¡¨å‘é‡...")
        
        # ä¸ºæ¯ä¸ªè¡¨ç”Ÿæˆæè¿°æ–‡æœ¬å¹¶å‘é‡åŒ–
        table_texts = []
        column_texts = []
        
        for table in self.tables:
            table_name = table.get("table_name", "")
            table_desc = table.get("table_description", "")
            columns = table.get("columns", [])
            
            # è¡¨çº§åˆ«æè¿°
            table_text = f"è¡¨å: {table_name}. æè¿°: {table_desc}"
            table_texts.append(table_text)
            
            # åˆ—çº§åˆ«æè¿°
            for col in columns:
                col_name = col.get("col", "")
                col_type = col.get("type", "")
                col_desc = col.get("description", "")
                
                col_text = f"è¡¨ {table_name} çš„å­—æ®µ {col_name} ({col_type}): {col_desc}"
                column_texts.append({
                    "text": col_text,
                    "table_name": table_name,
                    "column": col
                })
        
        # æ‰¹é‡å‘é‡åŒ–
        try:
            if table_texts:
                table_embeddings_list = [self.rag_engine._get_embedding(text) for text in table_texts]
                self.table_embeddings = np.array(table_embeddings_list)
                
            if column_texts:
                col_embeddings_list = [self.rag_engine._get_embedding(item["text"]) for item in column_texts]
                self.column_embeddings = {
                    "embeddings": np.array(col_embeddings_list),
                    "metadata": column_texts
                }
                
            print(f"âœ… [TableSelector] å‘é‡é¢„è®¡ç®—å®Œæˆ: {len(table_texts)} ä¸ªè¡¨, {len(column_texts)} ä¸ªå­—æ®µ")
            
        except Exception as e:
            print(f"âŒ [TableSelector] å‘é‡é¢„è®¡ç®—å¤±è´¥: {e}")
            self.table_embeddings = None
            self.column_embeddings = None
    
    def calculate_semantic_similarity(self, query: str, table_index: int) -> float:
        """è®¡ç®—æŸ¥è¯¢ä¸è¡¨çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if not self.rag_engine or self.table_embeddings is None:
            return 0.0
            
        try:
            # æŸ¥è¯¢å‘é‡åŒ–
            query_embedding = self.rag_engine._get_embedding(query)
            
            # è®¡ç®—ä¸æŒ‡å®šè¡¨çš„ä½™å¼¦ç›¸ä¼¼åº¦
            table_embedding = self.table_embeddings[table_index]
            similarity = np.dot(query_embedding, table_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(table_embedding) + 1e-10
            )
            
            return float(similarity)
            
        except Exception as e:
            print(f"âš ï¸ [TableSelector] è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def find_relevant_columns(self, query: str, table_name: str, top_k: int = 5) -> List[Dict]:
        """æ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„åˆ—"""
        if not self.column_embeddings:
            return []
            
        try:
            query_embedding = self.rag_engine._get_embedding(query)
            
            # ç­›é€‰å‡ºå±äºæŒ‡å®šè¡¨çš„åˆ—
            table_columns = []
            table_indices = []
            
            for i, col_meta in enumerate(self.column_embeddings["metadata"]):
                if col_meta["table_name"] == table_name:
                    table_columns.append(col_meta["column"])
                    table_indices.append(i)
            
            if not table_indices:
                return []
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            col_embeddings = self.column_embeddings["embeddings"][table_indices]
            similarities = np.dot(col_embeddings, query_embedding) / (
                np.linalg.norm(col_embeddings, axis=1) * np.linalg.norm(query_embedding) + 1e-10
            )
            
            # æ’åºå¹¶è¿”å›top-k
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            relevant_columns = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    col_info = table_columns[idx].copy()
                    col_info["similarity"] = float(similarities[idx])
                    relevant_columns.append(col_info)
            
            return relevant_columns
            
        except Exception as e:
            print(f"âš ï¸ [TableSelector] åˆ—ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            return []
    
    def analyze_query_intent(self, query: str) -> Dict:
        """åˆ†ææŸ¥è¯¢æ„å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸»è¦ä¾èµ–è¯­ä¹‰åŒ¹é…ï¼‰"""
        query_lower = query.lower()
        
        intent = {
            "has_aggregation": any(word in query_lower for word in ["æ€»", "å¹³å‡", "æœ€å¤§", "æœ€å°", "ç»Ÿè®¡", "è®¡ç®—", "sum", "avg", "max", "min", "count"]),
            "has_filtering": any(word in query_lower for word in ["where", "æ¡ä»¶", "ç­›é€‰", "è¿‡æ»¤", "ç­‰äº", "å¤§äº", "å°äº"]),
            "has_grouping": any(word in query_lower for word in ["æŒ‰", "åˆ†ç»„", "group by", "æ¯ä¸ª", "å„ä¸ª"]),
            "has_sorting": any(word in query_lower for word in ["æ’åº", "æ’å", "æœ€é«˜", "æœ€ä½", "order by", "top"]),
            "has_time": any(word in query_lower for word in ["å¹´", "æœˆ", "æ—¥", "æ—¶é—´", "æ—¥æœŸ", "yesterday", "today", "2023", "2024"]),
            "has_geography": any(word in query_lower for word in ["åŸå¸‚", "åœ°åŒº", "å›½å®¶", "çœ", "å·", "city", "country", "region"])
        }
        
        return intent
    
    def calculate_table_relevance(self, table: Dict, query: str, table_index: int) -> TableRelevance:
        """è®¡ç®—è¡¨çš„ç»¼åˆç›¸å…³æ€§å¾—åˆ†"""
        table_name = table.get("table_name", "")
        table_desc = table.get("table_description", "")
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦ (æƒé‡: 60%)
        semantic_score = self.calculate_semantic_similarity(query, table_index)
        
        # 2. å…³é”®è¯åŒ¹é… (æƒé‡: 25%)
        keyword_score = 0.0
        matched_keywords = []
        
        query_lower = query.lower()
        table_text = f"{table_name} {table_desc}".lower()
        
        # ç®€å•å…³é”®è¯åŒ¹é… - æ”¹è¿›ä¸­æ–‡åˆ†è¯
        query_words = []
        # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯
        for word in query_lower.split():
            query_words.append(word)
        
        # ä¸­æ–‡æŒ‰å­—ç¬¦åˆ†è¯ï¼ˆç®€å•æ–¹æ³•ï¼‰
        chinese_chars = []
        for char in query_lower:
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦èŒƒå›´
                chinese_chars.append(char)
        
        # ç»„åˆä¸­æ–‡è¯ï¼ˆ2-3å­—ç»„åˆï¼‰
        for i in range(len(chinese_chars)):
            if i + 1 < len(chinese_chars):
                query_words.append(chinese_chars[i] + chinese_chars[i+1])
            if i + 2 < len(chinese_chars):
                query_words.append(chinese_chars[i] + chinese_chars[i+1] + chinese_chars[i+2])
        
        for word in query_words:
            if len(word) > 1 and word in table_text:
                keyword_score += 0.1
                matched_keywords.append(word)
        
        # 3. åˆ—ç›¸å…³æ€§ (æƒé‡: 15%)
        relevant_columns = self.find_relevant_columns(query, table_name, top_k=3)
        column_score = min(len(relevant_columns) * 0.05, 0.15)
        
        # ç»¼åˆå¾—åˆ†
        if semantic_score > 0:
            # æœ‰è¯­ä¹‰åŒ¹é…æ—¶ä½¿ç”¨åŸå§‹æƒé‡
            total_score = (semantic_score * 0.6 + 
                          min(keyword_score, 0.25) * 0.25 + 
                          column_score * 0.15) * 100
        else:
            # æ— è¯­ä¹‰åŒ¹é…æ—¶æé«˜å…³é”®è¯å’Œåˆ—åŒ¹é…çš„æƒé‡
            total_score = (min(keyword_score, 0.6) * 0.7 + 
                          column_score * 0.3) * 100
        
        # ç”Ÿæˆæ¨ç†è¯´æ˜
        reasoning_parts = []
        if semantic_score > 0.3:
            reasoning_parts.append(f"è¯­ä¹‰åŒ¹é…åº¦é«˜ ({semantic_score:.2f})")
        if matched_keywords:
            reasoning_parts.append(f"å…³é”®è¯åŒ¹é…: {', '.join(matched_keywords[:3])}")
        if relevant_columns:
            col_names = [col.get('col', '') for col in relevant_columns[:2]]
            reasoning_parts.append(f"ç›¸å…³å­—æ®µ: {', '.join(col_names)}")
        
        reasoning = "; ".join(reasoning_parts) if reasoning_parts else "ç›¸å…³æ€§è¾ƒä½"
        
        return TableRelevance(
            table_name=table_name,
            table_description=table_desc,
            relevance_score=total_score,
            semantic_similarity=semantic_score,
            keyword_matches=matched_keywords,
            matched_columns=relevant_columns,
            reasoning=reasoning
        )
    
    def select_tables(self, query: str, top_k: int = 5) -> Tuple[List[TableRelevance], Dict]:
        """æ™ºèƒ½é€‰æ‹©ç›¸å…³è¡¨"""
        if not self.tables:
            return [], {"error": "æœªåŠ è½½ä»»ä½•æ•°æ®è¡¨schema"}
        
        start_time = time.perf_counter()
        
        # åˆ†ææŸ¥è¯¢æ„å›¾
        intent = self.analyze_query_intent(query)
        
        # è®¡ç®—æ¯ä¸ªè¡¨çš„ç›¸å…³æ€§
        table_relevances = []
        for i, table in enumerate(self.tables):
            relevance = self.calculate_table_relevance(table, query, i)
            table_relevances.append(relevance)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        table_relevances.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # é€‰æ‹©top-kä¸ªè¡¨
        selected_tables = table_relevances[:top_k]
        
        # è¿‡æ»¤æ‰å¾—åˆ†è¿‡ä½çš„è¡¨
        selected_tables = [t for t in selected_tables if t.relevance_score > 1.0]
        
        end_time = time.perf_counter()
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis_report = {
            "query": query,
            "intent": intent,
            "total_tables": len(self.tables),
            "selected_count": len(selected_tables),
            "processing_time_ms": (end_time - start_time) * 1000,
            "selection_reasoning": self._generate_selection_reasoning(selected_tables, intent),
            "use_semantic_matching": self.rag_engine is not None and self.table_embeddings is not None
        }
        
        return selected_tables, analysis_report
    
    def _generate_selection_reasoning(self, selected_tables: List[TableRelevance], intent: Dict) -> str:
        """ç”Ÿæˆè¡¨é€‰æ‹©æ¨ç†è¯´æ˜"""
        if not selected_tables:
            return "æœªæ‰¾åˆ°ç›¸å…³æ€§è¶³å¤Ÿé«˜çš„æ•°æ®è¡¨"
        
        reasoning_parts = []
        
        # æœ€é«˜ç›¸å…³æ€§è¡¨
        top_table = selected_tables[0]
        reasoning_parts.append(f"æœ€ç›¸å…³è¡¨: {top_table.table_name} (å¾—åˆ†: {top_table.relevance_score:.1f})")
        
        # è¯­ä¹‰åŒ¹é…æƒ…å†µ
        if hasattr(top_table, 'semantic_similarity') and top_table.semantic_similarity > 0.3:
            reasoning_parts.append(f"è¯­ä¹‰åŒ¹é…åº¦: {top_table.semantic_similarity:.2f}")
        
        # æŸ¥è¯¢æ„å›¾
        intent_features = [k.replace('has_', '') for k, v in intent.items() if v]
        if intent_features:
            reasoning_parts.append(f"æŸ¥è¯¢ç‰¹å¾: {', '.join(intent_features)}")
        
        return "; ".join(reasoning_parts)
    
    def get_table_context(self, selected_tables: List[TableRelevance]) -> str:
        """ç”Ÿæˆé€‰ä¸­è¡¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not selected_tables:
            return "æœªé€‰æ‹©ä»»ä½•è¡¨"
        
        context_parts = []
        
        for table_rel in selected_tables:
            table_info = f"è¡¨å: {table_rel.table_name}\n"
            table_info += f"æè¿°: {table_rel.table_description}\n"
            table_info += f"ç›¸å…³æ€§å¾—åˆ†: {table_rel.relevance_score:.1f}\n"
            
            if hasattr(table_rel, 'semantic_similarity'):
                table_info += f"è¯­ä¹‰ç›¸ä¼¼åº¦: {table_rel.semantic_similarity:.2f}\n"
            
            if table_rel.matched_columns:
                table_info += "ç›¸å…³å­—æ®µ:\n"
                for col in table_rel.matched_columns[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç›¸å…³å­—æ®µ
                    similarity_info = ""
                    if 'similarity' in col:
                        similarity_info = f" (ç›¸ä¼¼åº¦: {col['similarity']:.2f})"
                    table_info += f"  - {col.get('col', '')} ({col.get('type', '')}): {col.get('description', '')}{similarity_info}\n"
            
            context_parts.append(table_info)
        
        return "\n".join(context_parts)
    
    def update_schema(self, new_schema_paths: List[str]):
        """æ›´æ–°schemaé…ç½®ï¼ˆç”¨æˆ·é‡æ–°ä¸Šä¼ æ–‡ä»¶æ—¶è°ƒç”¨ï¼‰"""
        self.schema_paths = new_schema_paths
        self.load_dynamic_schema()
        
        # é‡æ–°è®¡ç®—å‘é‡
        if self.rag_engine and self.rag_engine.model:
            self.precompute_embeddings()


# æµ‹è¯•å‡½æ•°
def test_table_selector_with_rag():
    """æµ‹è¯•åŸºäºRAGçš„è¡¨é€‰æ‹©å™¨"""
    # è¿™é‡Œéœ€è¦ä¼ å…¥å®é™…çš„RAGå¼•æ“å®ä¾‹
    print("âš ï¸ æµ‹è¯•éœ€è¦åœ¨å®é™…ç¯å¢ƒä¸­è¿è¡Œï¼Œéœ€è¦RAGå¼•æ“å®ä¾‹")


if __name__ == "__main__":
    test_table_selector_with_rag()