"""
Intel DeepInsight æ™ºèƒ½å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
è‡ªåŠ¨è¯†åˆ«æ•°æ®å¼‚å¸¸ã€è¶‹åŠ¿å˜åŒ–å’Œä¸šåŠ¡é£é™©ç‚¹
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import json
import os
from dataclasses import dataclass

# å¼‚å¸¸ç±»å‹å›¾æ ‡æ˜ å°„
ANOMALY_TYPE_ICONS = {
    "statistical_outlier": "ğŸ“Š",
    "extreme_value": "âš ï¸", 
    "zero_value_anomaly": "ğŸ”´",
    "negative_profit": "ğŸ’¸",
    "high_profit_margin": "ğŸ“ˆ",
    "price_anomaly": "ğŸ’°",
    "trend_break": "ğŸ“‰",
    "declining_trend": "â¬‡ï¸"
}

# å¼‚å¸¸ç±»å‹ä¸­æ–‡åç§°æ˜ å°„
ANOMALY_TYPE_NAMES = {
    "statistical_outlier": "æ•°æ®å¼‚å¸¸",
    "extreme_value": "æå€¼å¼‚å¸¸", 
    "zero_value_anomaly": "é›¶å€¼å¼‚å¸¸",
    "negative_profit": "ä¸šåŠ¡å¼‚å¸¸",
    "high_profit_margin": "åˆ©æ¶¦å¼‚å¸¸",
    "price_anomaly": "ä»·æ ¼å¼‚å¸¸",
    "trend_break": "è¶‹åŠ¿å¼‚å¸¸",
    "declining_trend": "ä¸‹é™è¶‹åŠ¿"
}

@dataclass
class AnomalyPreview:
    """å¼‚å¸¸é¢„è§ˆæ•°æ®ç»“æ„"""
    type: str
    icon: str
    type_name: str
    short_description: str  # é™åˆ¶50å­—ç¬¦
    sample_data: List[str]  # 1-2ä¸ªæ•°æ®æ ·æœ¬
    impact_level: str  # "high", "medium", "low"
    confidence: float
    quick_reason: str  # ç®€è¦åŸå› è¯´æ˜
    quick_action: Optional[str] = None  # å¿«é€Ÿå¤„ç†å»ºè®®

class AnomalyDetector:
    """æ™ºèƒ½å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.anomaly_history_file = "data/anomaly_history.json"
        self._ensure_history_file()
    
    def _ensure_history_file(self):
        """ç¡®ä¿å¼‚å¸¸å†å²æ–‡ä»¶å­˜åœ¨"""
        os.makedirs("data", exist_ok=True)
        if not os.path.exists(self.anomaly_history_file):
            with open(self.anomaly_history_file, 'w', encoding='utf-8') as f:
                json.dump({"anomalies": []}, f, indent=2)
    
    def detect_statistical_anomalies(self, df: pd.DataFrame, query_context: str = "") -> List[Dict]:
        """æ£€æµ‹ç»Ÿè®¡å¼‚å¸¸"""
        anomalies = []
        
        if df.empty:
            return anomalies
        
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_cols:
                if df[col].notna().sum() < 3:  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œè·³è¿‡
                    continue
                
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                mean_val = df[col].mean()
                std_val = df[col].std()
                median_val = df[col].median()
                q1 = df[col].quantile(0.25)
                q3 = df[col].quantile(0.75)
                iqr = q3 - q1
                
                # æ£€æµ‹ç¦»ç¾¤å€¼ (IQRæ–¹æ³•)
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                
                if len(outliers) > 0:
                    outlier_values = outliers[col].tolist()
                    # è·å–å¼‚å¸¸å€¼çš„è¡Œç´¢å¼•å’Œå®Œæ•´è®°å½•
                    outlier_indices = outliers.index.tolist()
                    outlier_records = []
                    for idx in outlier_indices[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå®Œæ•´è®°å½•
                        record = df.loc[idx].to_dict()
                        outlier_records.append({
                            "row_index": idx,
                            "anomaly_value": record[col],
                            "full_record": {k: v for k, v in record.items() if pd.notna(v)}
                        })
                    
                    anomalies.append({
                        "type": "statistical_outlier",
                        "column": col,
                        "severity": "medium",
                        "count": len(outliers),
                        "values": outlier_values[:5],  # åªæ˜¾ç¤ºå‰5ä¸ª
                        "description": f"{col} åˆ—å‘ç° {len(outliers)} ä¸ªç»Ÿè®¡å¼‚å¸¸å€¼",
                        "details": f"æ­£å¸¸èŒƒå›´: {lower_bound:.2f} - {upper_bound:.2f}",
                        "suggestion": f"å»ºè®®æ£€æŸ¥è¿™äº›å¼‚å¸¸å€¼æ˜¯å¦ä¸ºæ•°æ®é”™è¯¯æˆ–ç‰¹æ®Šæƒ…å†µ",
                        "criteria": {
                            "method": "IQR (å››åˆ†ä½è·) æ–¹æ³•",
                            "threshold": "1.5å€IQR",
                            "q1": q1,
                            "q3": q3,
                            "iqr": iqr,
                            "lower_bound": lower_bound,
                            "upper_bound": upper_bound
                        },
                        "evidence": {
                            "outlier_records": outlier_records,
                            "statistical_summary": {
                                "mean": mean_val,
                                "median": median_val,
                                "std": std_val,
                                "min_outlier": min(outlier_values),
                                "max_outlier": max(outlier_values)
                            }
                        }
                    })
                
                # æ£€æµ‹æå€¼
                if std_val > 0:
                    z_scores = np.abs((df[col] - mean_val) / std_val)
                    extreme_values = df[z_scores > 3]  # Z-score > 3
                    
                    if len(extreme_values) > 0:
                        extreme_indices = extreme_values.index.tolist()
                        extreme_records = []
                        for idx in extreme_indices[:3]:
                            record = df.loc[idx].to_dict()
                            z_score = abs((record[col] - mean_val) / std_val)
                            extreme_records.append({
                                "row_index": idx,
                                "anomaly_value": record[col],
                                "z_score": z_score,
                                "full_record": {k: v for k, v in record.items() if pd.notna(v)}
                            })
                        
                        anomalies.append({
                            "type": "extreme_value",
                            "column": col,
                            "severity": "high",
                            "count": len(extreme_values),
                            "values": extreme_values[col].tolist()[:3],
                            "description": f"{col} åˆ—å‘ç° {len(extreme_values)} ä¸ªæç«¯å€¼",
                            "details": f"å‡å€¼: {mean_val:.2f}, æ ‡å‡†å·®: {std_val:.2f}",
                            "suggestion": "æç«¯å€¼å¯èƒ½è¡¨ç¤ºå¼‚å¸¸ä¸šåŠ¡æƒ…å†µï¼Œå»ºè®®æ·±å…¥åˆ†æ",
                            "criteria": {
                                "method": "Z-Score æ ‡å‡†åŒ–æ–¹æ³•",
                                "threshold": "Z-Score > 3",
                                "mean": mean_val,
                                "std": std_val,
                                "z_threshold": 3.0
                            },
                            "evidence": {
                                "extreme_records": extreme_records,
                                "statistical_summary": {
                                    "mean": mean_val,
                                    "std": std_val,
                                    "max_z_score": max([abs((val - mean_val) / std_val) for val in extreme_values[col]]),
                                    "extreme_range": f"{extreme_values[col].min():.2f} - {extreme_values[col].max():.2f}"
                                }
                            }
                        })
                
                # æ£€æµ‹é›¶å€¼å¼‚å¸¸
                zero_count = (df[col] == 0).sum()
                zero_ratio = zero_count / len(df)
                if zero_ratio > 0.3 and col in ['sales', 'profit', 'é”€å”®é¢', 'åˆ©æ¶¦']:
                    zero_records = df[df[col] == 0]
                    zero_sample_records = []
                    for idx in zero_records.index[:3]:
                        record = df.loc[idx].to_dict()
                        zero_sample_records.append({
                            "row_index": idx,
                            "full_record": {k: v for k, v in record.items() if pd.notna(v)}
                        })
                    
                    anomalies.append({
                        "type": "zero_value_anomaly",
                        "column": col,
                        "severity": "medium",
                        "count": zero_count,
                        "ratio": zero_ratio,
                        "description": f"{col} åˆ—æœ‰ {zero_ratio:.1%} çš„æ•°æ®ä¸ºé›¶",
                        "details": f"é›¶å€¼æ•°é‡: {zero_count}/{len(df)}",
                        "suggestion": "å¤§é‡é›¶å€¼å¯èƒ½è¡¨ç¤ºä¸šåŠ¡é—®é¢˜æˆ–æ•°æ®è´¨é‡é—®é¢˜",
                        "criteria": {
                            "method": "é›¶å€¼æ¯”ä¾‹æ£€æµ‹",
                            "threshold": "30%",
                            "actual_ratio": zero_ratio,
                            "threshold_ratio": 0.3
                        },
                        "evidence": {
                            "zero_records": zero_sample_records,
                            "distribution": {
                                "total_records": len(df),
                                "zero_count": zero_count,
                                "non_zero_count": len(df) - zero_count,
                                "zero_percentage": zero_ratio * 100
                            }
                        }
                    })
        
        except Exception as e:
            print(f"ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
        
        return anomalies
    
    def detect_business_anomalies(self, df: pd.DataFrame, query_context: str = "") -> List[Dict]:
        """æ£€æµ‹ä¸šåŠ¡å¼‚å¸¸"""
        anomalies = []
        
        if df.empty:
            return anomalies
        
        try:
            # æ£€æµ‹è´Ÿåˆ©æ¶¦
            if 'profit' in df.columns or 'åˆ©æ¶¦' in df.columns:
                profit_col = 'profit' if 'profit' in df.columns else 'åˆ©æ¶¦'
                negative_profit = df[df[profit_col] < 0]
                
                if len(negative_profit) > 0:
                    total_negative = negative_profit[profit_col].sum()
                    negative_sample_records = []
                    for idx in negative_profit.index[:3]:
                        record = df.loc[idx].to_dict()
                        negative_sample_records.append({
                            "row_index": idx,
                            "profit_value": record[profit_col],
                            "full_record": {k: v for k, v in record.items() if pd.notna(v)}
                        })
                    
                    anomalies.append({
                        "type": "negative_profit",
                        "column": profit_col,
                        "severity": "high",
                        "count": len(negative_profit),
                        "total_loss": abs(total_negative),
                        "description": f"å‘ç° {len(negative_profit)} æ¡è´Ÿåˆ©æ¶¦è®°å½•",
                        "details": f"æ€»æŸå¤±: {abs(total_negative):,.2f}",
                        "suggestion": "è´Ÿåˆ©æ¶¦å¯èƒ½è¡¨ç¤ºå®šä»·ç­–ç•¥é—®é¢˜æˆ–æˆæœ¬æ§åˆ¶ä¸å½“",
                        "criteria": {
                            "method": "ä¸šåŠ¡è§„åˆ™æ£€æµ‹",
                            "threshold": "åˆ©æ¶¦ < 0",
                            "business_rule": "æ­£å¸¸ä¸šåŠ¡ä¸­åˆ©æ¶¦åº”ä¸ºæ­£å€¼"
                        },
                        "evidence": {
                            "negative_records": negative_sample_records,
                            "financial_impact": {
                                "total_loss": abs(total_negative),
                                "average_loss": abs(total_negative) / len(negative_profit),
                                "worst_loss": negative_profit[profit_col].min(),
                                "affected_percentage": len(negative_profit) / len(df) * 100
                            }
                        }
                    })
            
            # æ£€æµ‹å¼‚å¸¸é«˜çš„åˆ©æ¶¦ç‡
            if all(col in df.columns for col in ['sales', 'profit']) or all(col in df.columns for col in ['é”€å”®é¢', 'åˆ©æ¶¦']):
                sales_col = 'sales' if 'sales' in df.columns else 'é”€å”®é¢'
                profit_col = 'profit' if 'profit' in df.columns else 'åˆ©æ¶¦'
                
                # è®¡ç®—åˆ©æ¶¦ç‡
                df_temp = df[(df[sales_col] > 0) & (df[profit_col].notna())].copy()
                if not df_temp.empty:
                    df_temp['profit_margin'] = df_temp[profit_col] / df_temp[sales_col]
                    
                    # æ£€æµ‹å¼‚å¸¸é«˜çš„åˆ©æ¶¦ç‡ (>100%)
                    high_margin = df_temp[df_temp['profit_margin'] > 1.0]
                    if len(high_margin) > 0:
                        high_margin_sample_records = []
                        for idx in high_margin.index[:3]:
                            record = df.loc[idx].to_dict()
                            high_margin_sample_records.append({
                                "row_index": idx,
                                "profit_margin": record.get('profit_margin', 0),
                                "sales": record.get(sales_col, 0),
                                "profit": record.get(profit_col, 0),
                                "full_record": {k: v for k, v in record.items() if pd.notna(v) and k != 'profit_margin'}
                            })
                        
                        anomalies.append({
                            "type": "high_profit_margin",
                            "column": "profit_margin",
                            "severity": "medium",
                            "count": len(high_margin),
                            "max_margin": high_margin['profit_margin'].max(),
                            "description": f"å‘ç° {len(high_margin)} æ¡åˆ©æ¶¦ç‡è¶…è¿‡100%çš„è®°å½•",
                            "details": f"æœ€é«˜åˆ©æ¶¦ç‡: {high_margin['profit_margin'].max():.1%}",
                            "suggestion": "å¼‚å¸¸é«˜çš„åˆ©æ¶¦ç‡å¯èƒ½è¡¨ç¤ºæ•°æ®é”™è¯¯æˆ–ç‰¹æ®Šä¸šåŠ¡æƒ…å†µ",
                            "criteria": {
                                "method": "åˆ©æ¶¦ç‡è®¡ç®—æ£€æµ‹",
                                "threshold": "åˆ©æ¶¦ç‡ > 100%",
                                "calculation": "åˆ©æ¶¦ç‡ = åˆ©æ¶¦ / é”€å”®é¢",
                                "normal_range": "é€šå¸¸åˆ©æ¶¦ç‡åœ¨5%-50%ä¹‹é—´"
                            },
                            "evidence": {
                                "high_margin_records": high_margin_sample_records,
                                "margin_statistics": {
                                    "max_margin": high_margin['profit_margin'].max(),
                                    "min_margin": high_margin['profit_margin'].min(),
                                    "avg_margin": high_margin['profit_margin'].mean(),
                                    "affected_percentage": len(high_margin) / len(df_temp) * 100
                                }
                            }
                        })
            
            # æ£€æµ‹é”€å”®é¢ä¸æ•°é‡çš„ä¸ä¸€è‡´
            if all(col in df.columns for col in ['sales', 'quantity']) or all(col in df.columns for col in ['é”€å”®é¢', 'æ•°é‡']):
                sales_col = 'sales' if 'sales' in df.columns else 'é”€å”®é¢'
                qty_col = 'quantity' if 'quantity' in df.columns else 'æ•°é‡'
                
                # è®¡ç®—å•ä»·
                df_temp = df[(df[sales_col] > 0) & (df[qty_col] > 0)].copy()
                if not df_temp.empty:
                    df_temp['unit_price'] = df_temp[sales_col] / df_temp[qty_col]
                    
                    # æ£€æµ‹å¼‚å¸¸å•ä»·
                    q1 = df_temp['unit_price'].quantile(0.25)
                    q3 = df_temp['unit_price'].quantile(0.75)
                    iqr = q3 - q1
                    
                    if iqr > 0:
                        lower_bound = q1 - 3 * iqr  # æ›´å®½æ¾çš„ç•Œé™
                        upper_bound = q3 + 3 * iqr
                        
                        price_anomalies = df_temp[(df_temp['unit_price'] < lower_bound) | (df_temp['unit_price'] > upper_bound)]
                        
                        if len(price_anomalies) > 0:
                            price_sample_records = []
                            for idx in price_anomalies.index[:3]:
                                record = df.loc[idx].to_dict()
                                price_sample_records.append({
                                    "row_index": idx,
                                    "unit_price": record.get('unit_price', 0),
                                    "sales": record.get(sales_col, 0),
                                    "quantity": record.get(qty_col, 0),
                                    "full_record": {k: v for k, v in record.items() if pd.notna(v) and k != 'unit_price'}
                                })
                            
                            anomalies.append({
                                "type": "price_anomaly",
                                "column": "unit_price",
                                "severity": "medium",
                                "count": len(price_anomalies),
                                "price_range": f"{price_anomalies['unit_price'].min():.2f} - {price_anomalies['unit_price'].max():.2f}",
                                "description": f"å‘ç° {len(price_anomalies)} æ¡å¼‚å¸¸å•ä»·è®°å½•",
                                "details": f"æ­£å¸¸å•ä»·èŒƒå›´: {lower_bound:.2f} - {upper_bound:.2f}",
                                "suggestion": "å¼‚å¸¸å•ä»·å¯èƒ½è¡¨ç¤ºæŠ˜æ‰£ã€ä¿ƒé”€æˆ–æ•°æ®å½•å…¥é”™è¯¯",
                                "criteria": {
                                    "method": "IQR (å››åˆ†ä½è·) æ–¹æ³•",
                                    "threshold": "3å€IQR",
                                    "calculation": "å•ä»· = é”€å”®é¢ / æ•°é‡",
                                    "q1": q1,
                                    "q3": q3,
                                    "iqr": iqr,
                                    "lower_bound": lower_bound,
                                    "upper_bound": upper_bound
                                },
                                "evidence": {
                                    "price_anomaly_records": price_sample_records,
                                    "price_statistics": {
                                        "normal_price_mean": df_temp['unit_price'].mean(),
                                        "normal_price_median": df_temp['unit_price'].median(),
                                        "anomaly_price_min": price_anomalies['unit_price'].min(),
                                        "anomaly_price_max": price_anomalies['unit_price'].max(),
                                        "affected_percentage": len(price_anomalies) / len(df_temp) * 100
                                    }
                                }
                            })
        
        except Exception as e:
            print(f"ä¸šåŠ¡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
        
        return anomalies
    
    def detect_trend_anomalies(self, df: pd.DataFrame, query_context: str = "") -> List[Dict]:
        """æ£€æµ‹è¶‹åŠ¿å¼‚å¸¸"""
        anomalies = []
        
        if df.empty or len(df) < 5:  # æ•°æ®ç‚¹å¤ªå°‘æ— æ³•åˆ†æè¶‹åŠ¿
            return anomalies
        
        try:
            # å¯»æ‰¾æ—¶é—´åˆ— - æ”¯æŒdatetimeç±»å‹å’Œå¯è½¬æ¢çš„å­—ç¬¦ä¸²
            date_columns = []
            for col in df.columns:
                # æ£€æŸ¥datetimeç±»å‹
                if pd.api.types.is_datetime64_any_dtype(df[col]):
                    date_columns.append(col)
                # æ£€æŸ¥å¯è½¬æ¢çš„å­—ç¬¦ä¸²
                elif df[col].dtype == 'object':
                    try:
                        pd.to_datetime(df[col].head())
                        date_columns.append(col)
                    except:
                        pass
            
            # å¦‚æœæ²¡æœ‰æ—¶é—´åˆ—ï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•ä½œä¸ºæ—¶é—´åºåˆ—
            if not date_columns:
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹è¿›è¡Œè¶‹åŠ¿åˆ†æ
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0 and len(df) >= 7:
                    # ä½¿ç”¨ç´¢å¼•ä½œä¸ºæ—¶é—´åºåˆ—è¿›è¡Œè¶‹åŠ¿åˆ†æ
                    for num_col in numeric_cols:
                        if df[num_col].notna().sum() < 7:
                            continue
                        
                        values = df[num_col].dropna()
                        if len(values) < 7:
                            continue
                        
                        # æ£€æµ‹æŒç»­ä¸‹é™è¶‹åŠ¿ï¼ˆæ— éœ€æ—¶é—´åˆ—ï¼‰
                        recent_values = values.tail(10)  # å–æœ€å10ä¸ªå€¼
                        if len(recent_values) >= 7:
                            # æ£€æŸ¥æ˜¯å¦è¿ç»­ä¸‹é™
                            consecutive_declines = 0
                            for i in range(len(recent_values)-1):
                                if recent_values.iloc[i] > recent_values.iloc[i+1]:
                                    consecutive_declines += 1
                                else:
                                    break
                            
                            if consecutive_declines >= 6:  # è‡³å°‘è¿ç»­6æ¬¡ä¸‹é™
                                decline_rate = (recent_values.iloc[-1] - recent_values.iloc[0]) / recent_values.iloc[0] if recent_values.iloc[0] != 0 else 0
                                if decline_rate < -0.15:  # ä¸‹é™è¶…è¿‡15%
                                    # è®°å½•ä¸‹é™è¶‹åŠ¿çš„è¯¦ç»†ä¿¡æ¯
                                    decline_records = []
                                    for i in range(len(recent_values)):
                                        decline_records.append({
                                            "period": i + 1,
                                            "index": recent_values.index[i],
                                            "value": recent_values.iloc[i],
                                            "cumulative_decline": (recent_values.iloc[i] - recent_values.iloc[0]) / recent_values.iloc[0] if recent_values.iloc[0] != 0 else 0
                                        })
                                    
                                    anomalies.append({
                                        "type": "declining_trend",
                                        "column": num_col,
                                        "severity": "high",
                                        "decline_rate": abs(decline_rate),
                                        "description": f"{num_col} å‘ˆç°æŒç»­ä¸‹é™è¶‹åŠ¿",
                                        "details": f"è¿‘æœŸä¸‹é™å¹…åº¦: {abs(decline_rate):.1%}",
                                        "suggestion": "æŒç»­ä¸‹é™è¶‹åŠ¿éœ€è¦å…³æ³¨ï¼Œå¯èƒ½éœ€è¦é‡‡å–å¹²é¢„æªæ–½",
                                        "criteria": {
                                            "method": "è¿ç»­ä¸‹é™æ£€æµ‹",
                                            "threshold": f"è¿ç»­{consecutive_declines}æœŸä¸‹é™ä¸”æ€»é™å¹… > 15%",
                                            "observation_window": f"æœ€è¿‘{len(recent_values)}ä¸ªæ•°æ®ç‚¹",
                                            "decline_threshold": 0.15
                                        },
                                        "evidence": {
                                            "decline_sequence": decline_records,
                                            "trend_statistics": {
                                                "total_decline_rate": decline_rate,
                                                "periods_analyzed": len(recent_values),
                                                "start_value": recent_values.iloc[0],
                                                "end_value": recent_values.iloc[-1],
                                                "average_period_decline": decline_rate / (len(recent_values) - 1),
                                                "consecutive_declines": consecutive_declines
                                            }
                                        }
                                    })
                return anomalies
            
            date_col = date_columns[0]
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            for num_col in numeric_cols:
                if df[num_col].notna().sum() < 5:
                    continue
                
                # æŒ‰æ—¶é—´æ’åº
                df_sorted = df.sort_values(date_col)
                values = df_sorted[num_col].dropna()
                
                if len(values) < 5:
                    continue
                
                # è®¡ç®—ç§»åŠ¨å¹³å‡å’Œæ ‡å‡†å·®
                window_size = min(5, len(values) // 2)
                rolling_mean = values.rolling(window=window_size).mean()
                rolling_std = values.rolling(window=window_size).std()
                
                # æ£€æµ‹çªç„¶çš„å¤§å¹…å˜åŒ–
                pct_change = values.pct_change().abs()
                large_changes = pct_change[pct_change > 0.5]  # 50%ä»¥ä¸Šçš„å˜åŒ–
                
                if len(large_changes) > 0:
                    # æ‰¾åˆ°å˜åŒ–æœ€å¤§çš„å‡ ä¸ªç‚¹
                    change_records = []
                    for idx in pct_change.nlargest(3).index:
                        if idx > 0:  # ç¡®ä¿æœ‰å‰ä¸€ä¸ªå€¼è¿›è¡Œæ¯”è¾ƒ
                            prev_idx = values.index[values.index.get_loc(idx) - 1]
                            current_val = values.loc[idx]
                            prev_val = values.loc[prev_idx]
                            change_pct = (current_val - prev_val) / prev_val if prev_val != 0 else float('inf')
                            
                            change_records.append({
                                "current_index": idx,
                                "previous_index": prev_idx,
                                "current_value": current_val,
                                "previous_value": prev_val,
                                "change_percentage": change_pct,
                                "change_absolute": current_val - prev_val
                            })
                    
                    anomalies.append({
                        "type": "trend_break",
                        "column": num_col,
                        "severity": "medium",
                        "count": len(large_changes),
                        "max_change": large_changes.max(),
                        "description": f"{num_col} è¶‹åŠ¿ä¸­å‘ç° {len(large_changes)} ä¸ªçªå˜ç‚¹",
                        "details": f"æœ€å¤§å˜åŒ–å¹…åº¦: {large_changes.max():.1%}",
                        "suggestion": "è¶‹åŠ¿çªå˜å¯èƒ½è¡¨ç¤ºå¸‚åœºå˜åŒ–ã€æ”¿ç­–å½±å“æˆ–å¼‚å¸¸äº‹ä»¶",
                        "criteria": {
                            "method": "ç™¾åˆ†æ¯”å˜åŒ–æ£€æµ‹",
                            "threshold": "å˜åŒ–å¹…åº¦ > 50%",
                            "calculation": "å˜åŒ–ç‡ = (å½“å‰å€¼ - å‰å€¼) / å‰å€¼",
                            "time_window": "é€æœŸæ¯”è¾ƒ"
                        },
                        "evidence": {
                            "trend_break_points": change_records,
                            "trend_statistics": {
                                "total_data_points": len(values),
                                "break_points_count": len(large_changes),
                                "max_change_percentage": large_changes.max(),
                                "avg_change_percentage": large_changes.mean(),
                                "break_frequency": len(large_changes) / len(values) * 100
                            }
                        }
                    })
                
                # æ£€æµ‹æŒç»­ä¸‹é™è¶‹åŠ¿
                if len(values) >= 7:
                    recent_values = values.tail(7)
                    if all(recent_values.iloc[i] >= recent_values.iloc[i+1] for i in range(len(recent_values)-1)):
                        decline_rate = (recent_values.iloc[-1] - recent_values.iloc[0]) / recent_values.iloc[0]
                        if decline_rate < -0.2:  # ä¸‹é™è¶…è¿‡20%
                            # è®°å½•ä¸‹é™è¶‹åŠ¿çš„è¯¦ç»†ä¿¡æ¯
                            decline_records = []
                            for i, idx in enumerate(recent_values.index):
                                decline_records.append({
                                    "period": i + 1,
                                    "index": idx,
                                    "value": recent_values.iloc[i],
                                    "cumulative_decline": (recent_values.iloc[i] - recent_values.iloc[0]) / recent_values.iloc[0] if recent_values.iloc[0] != 0 else 0
                                })
                            
                            anomalies.append({
                                "type": "declining_trend",
                                "column": num_col,
                                "severity": "high",
                                "decline_rate": abs(decline_rate),
                                "description": f"{num_col} å‘ˆç°æŒç»­ä¸‹é™è¶‹åŠ¿",
                                "details": f"è¿‘æœŸä¸‹é™å¹…åº¦: {abs(decline_rate):.1%}",
                                "suggestion": "æŒç»­ä¸‹é™è¶‹åŠ¿éœ€è¦å…³æ³¨ï¼Œå¯èƒ½éœ€è¦é‡‡å–å¹²é¢„æªæ–½",
                                "criteria": {
                                    "method": "è¿ç»­ä¸‹é™æ£€æµ‹",
                                    "threshold": "è¿ç»­7æœŸä¸‹é™ä¸”æ€»é™å¹… > 20%",
                                    "observation_window": "æœ€è¿‘7ä¸ªæ•°æ®ç‚¹",
                                    "decline_threshold": 0.2
                                },
                                "evidence": {
                                    "decline_sequence": decline_records,
                                    "trend_statistics": {
                                        "total_decline_rate": decline_rate,
                                        "periods_analyzed": len(recent_values),
                                        "start_value": recent_values.iloc[0],
                                        "end_value": recent_values.iloc[-1],
                                        "average_period_decline": decline_rate / (len(recent_values) - 1),
                                        "consecutive_declines": len(recent_values) - 1
                                    }
                                }
                            })
        
        except Exception as e:
            print(f"è¶‹åŠ¿å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
        
        return anomalies
    
    def analyze_anomalies(self, df: pd.DataFrame, query_context: str = "") -> Dict:
        """ç»¼åˆå¼‚å¸¸åˆ†æ"""
        all_anomalies = []
        
        # æ”¶é›†å„ç±»å¼‚å¸¸
        all_anomalies.extend(self.detect_statistical_anomalies(df, query_context))
        all_anomalies.extend(self.detect_business_anomalies(df, query_context))
        all_anomalies.extend(self.detect_trend_anomalies(df, query_context))
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {"high": 3, "medium": 2, "low": 1}
        all_anomalies.sort(key=lambda x: severity_order.get(x.get("severity", "low"), 1), reverse=True)
        
        # ä¸ºæ¯ä¸ªå¼‚å¸¸æ·»åŠ æ›´è¯¦ç»†çš„ä¾æ®ä¿¡æ¯
        for anomaly in all_anomalies:
            anomaly_type = anomaly.get('type', 'unknown')
            column = anomaly.get('column', 'æœªçŸ¥åˆ—')
            
            # æ ¹æ®å¼‚å¸¸ç±»å‹æ·»åŠ æ›´è¯¦ç»†çš„ä¾æ®æè¿°
            if 'evidence' in anomaly:
                evidence = anomaly['evidence']
                
                # ç»Ÿè®¡å¼‚å¸¸çš„ä¾æ®
                if anomaly_type == 'statistical_outlier':
                    if 'outlier_records' in evidence and evidence['outlier_records']:
                        records = evidence['outlier_records'][:2]  # å–å‰2ä¸ªç¤ºä¾‹
                        details = "**å…·ä½“å¼‚å¸¸æ•°æ®ç¤ºä¾‹**:\n"
                        for rec in records:
                            details += f"â€¢ è¡Œ{rec['row_index']}: å€¼={rec['anomaly_value']:.2f}\n"
                            if 'full_record' in rec:
                                details += f"  å®Œæ•´è®°å½•: {str(rec['full_record'])[:100]}...\n"
                        anomaly['evidence_details'] = details
                
                elif anomaly_type == 'negative_profit':
                    if 'negative_records' in evidence and evidence['negative_records']:
                        records = evidence['negative_records'][:2]
                        details = "**è´Ÿåˆ©æ¶¦å…·ä½“ç¤ºä¾‹**:\n"
                        for rec in records:
                            details += f"â€¢ è¡Œ{rec['row_index']}: åˆ©æ¶¦={rec['profit_value']:.2f}\n"
                        anomaly['evidence_details'] = details
                
                elif anomaly_type == 'extreme_value':
                    if 'extreme_records' in evidence and evidence['extreme_records']:
                        records = evidence['extreme_records'][:2]
                        details = "**æç«¯å€¼å…·ä½“ç¤ºä¾‹**:\n"
                        for rec in records:
                            details += f"â€¢ è¡Œ{rec['row_index']}: å€¼={rec['anomaly_value']:.2f}, Zåˆ†æ•°={rec['z_score']:.2f}\n"
                        anomaly['evidence_details'] = details
                
                # æ·»åŠ ç»Ÿè®¡ä¾æ®
                if 'criteria' in anomaly:
                    criteria = anomaly['criteria']
                    if anomaly_type == 'statistical_outlier':
                        anomaly['statistical_basis'] = f"æ£€æµ‹æ–¹æ³•: {criteria.get('method', 'IQRæ–¹æ³•')}, æ­£å¸¸èŒƒå›´: {criteria.get('lower_bound', 0):.2f} - {criteria.get('upper_bound', 0):.2f}"
                    elif anomaly_type == 'extreme_value':
                        anomaly['statistical_basis'] = f"æ£€æµ‹æ–¹æ³•: {criteria.get('method', 'Z-Scoreæ–¹æ³•')}, é˜ˆå€¼: Z > {criteria.get('z_threshold', 3)}"
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            "total_anomalies": len(all_anomalies),
            "high_severity": len([a for a in all_anomalies if a.get("severity") == "high"]),
            "medium_severity": len([a for a in all_anomalies if a.get("severity") == "medium"]),
            "low_severity": len([a for a in all_anomalies if a.get("severity") == "low"]),
            "anomalies": all_anomalies,
            "analysis_time": datetime.now().isoformat()
        }
        
        return summary
    
    def _save_anomaly_to_history(self, analysis: Dict, query_context: str):
        """ä¿å­˜å¼‚å¸¸åˆ†æåˆ°å†å²è®°å½•"""
        try:
            with open(self.anomaly_history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ·»åŠ æ–°çš„å¼‚å¸¸è®°å½•
            history_entry = {
                "timestamp": datetime.now().isoformat(),
                "query_context": query_context,
                "total_anomalies": analysis["total_anomalies"],
                "high_severity": analysis["high_severity"],
                "anomaly_types": list(set([a["type"] for a in analysis["anomalies"]]))
            }
            
            data["anomalies"].append(history_entry)
            
            # ä¿æŒæœ€è¿‘100æ¡è®°å½•
            if len(data["anomalies"]) > 100:
                data["anomalies"] = data["anomalies"][-100:]
            
            with open(self.anomaly_history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        
        except Exception as e:
            print(f"ä¿å­˜å¼‚å¸¸å†å²å¤±è´¥: {e}")
    
    def _generate_anomaly_preview(self, anomaly: Dict) -> AnomalyPreview:
        """ç”Ÿæˆå¼‚å¸¸é¢„è§ˆæ•°æ®"""
        anomaly_type = anomaly.get('type', 'unknown')
        icon = ANOMALY_TYPE_ICONS.get(anomaly_type, "ğŸ”")
        type_name = ANOMALY_TYPE_NAMES.get(anomaly_type, "æœªçŸ¥å¼‚å¸¸")
        
        # ç”Ÿæˆç®€çŸ­æè¿°ï¼ˆé™åˆ¶50å­—ç¬¦ï¼‰
        description = anomaly.get('description', '')
        short_description = description[:47] + "..." if len(description) > 50 else description
        
        # ç”Ÿæˆæ•°æ®æ ·æœ¬
        sample_data = []
        if 'evidence' in anomaly:
            evidence = anomaly['evidence']
            
            # ä»ä¸åŒç±»å‹çš„è¯æ®ä¸­æå–æ ·æœ¬
            if 'outlier_records' in evidence and evidence['outlier_records']:
                for record in evidence['outlier_records'][:2]:
                    sample_data.append(f"è¡Œ{record['row_index']}: {record['anomaly_value']:.2f}")
            
            elif 'negative_records' in evidence and evidence['negative_records']:
                for record in evidence['negative_records'][:2]:
                    sample_data.append(f"è¡Œ{record['row_index']}: åˆ©æ¶¦{record['profit_value']:.2f}")
            
            elif 'extreme_records' in evidence and evidence['extreme_records']:
                for record in evidence['extreme_records'][:2]:
                    sample_data.append(f"è¡Œ{record['row_index']}: {record['anomaly_value']:.2f}")
            
            elif 'zero_records' in evidence and evidence['zero_records']:
                for record in evidence['zero_records'][:2]:
                    sample_data.append(f"è¡Œ{record['row_index']}: é›¶å€¼")
        
        # å¦‚æœæ²¡æœ‰å…·ä½“æ ·æœ¬ï¼Œä½¿ç”¨å¼‚å¸¸å€¼
        if not sample_data and 'values' in anomaly:
            values = anomaly['values'][:2]
            sample_data = [f"{val:.2f}" if isinstance(val, (int, float)) else str(val) for val in values]
        
        # ç”Ÿæˆç®€è¦åŸå› è¯´æ˜
        quick_reason = self._generate_quick_reason(anomaly)
        
        # ç”Ÿæˆå¿«é€Ÿå¤„ç†å»ºè®®
        quick_action = self._generate_quick_action(anomaly)
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        confidence = self._calculate_confidence(anomaly)
        
        return AnomalyPreview(
            type=anomaly_type,
            icon=icon,
            type_name=type_name,
            short_description=short_description,
            sample_data=sample_data,
            impact_level=anomaly.get('severity', 'medium'),
            confidence=confidence,
            quick_reason=quick_reason,
            quick_action=quick_action
        )
    
    def _generate_quick_reason(self, anomaly: Dict) -> str:
        """ç”Ÿæˆç®€è¦åŸå› è¯´æ˜"""
        anomaly_type = anomaly.get('type', '')
        
        if anomaly_type == 'statistical_outlier':
            if 'criteria' in anomaly:
                criteria = anomaly['criteria']
                lower = criteria.get('lower_bound', 0)
                upper = criteria.get('upper_bound', 0)
                return f"æ•°å€¼è¶…å‡ºæ­£å¸¸èŒƒå›´ {lower:.1f}-{upper:.1f}"
        
        elif anomaly_type == 'extreme_value':
            if 'criteria' in anomaly:
                threshold = anomaly['criteria'].get('z_threshold', 3)
                return f"Z-Scoreè¶…è¿‡{threshold}å€æ ‡å‡†å·®"
        
        elif anomaly_type == 'negative_profit':
            count = anomaly.get('count', 0)
            return f"å‘ç°{count}æ¡è´Ÿåˆ©æ¶¦è®°å½•"
        
        elif anomaly_type == 'high_profit_margin':
            max_margin = anomaly.get('max_margin', 0)
            return f"åˆ©æ¶¦ç‡é«˜è¾¾{max_margin:.1%}"
        
        elif anomaly_type == 'zero_value_anomaly':
            ratio = anomaly.get('ratio', 0)
            return f"{ratio:.1%}çš„æ•°æ®ä¸ºé›¶å€¼"
        
        elif anomaly_type == 'price_anomaly':
            return "å•ä»·å¼‚å¸¸åç¦»æ­£å¸¸èŒƒå›´"
        
        elif anomaly_type == 'trend_break':
            max_change = anomaly.get('max_change', 0)
            return f"è¶‹åŠ¿çªå˜å¹…åº¦è¾¾{max_change:.1%}"
        
        elif anomaly_type == 'declining_trend':
            decline_rate = anomaly.get('decline_rate', 0)
            return f"æŒç»­ä¸‹é™{decline_rate:.1%}"
        
        return "æ£€æµ‹åˆ°æ•°æ®å¼‚å¸¸"
    
    def _generate_quick_action(self, anomaly: Dict) -> Optional[str]:
        """ç”Ÿæˆå¿«é€Ÿå¤„ç†å»ºè®®"""
        severity = anomaly.get('severity', 'medium')
        anomaly_type = anomaly.get('type', '')
        
        if severity == 'high':
            if anomaly_type == 'negative_profit':
                return "ç«‹å³æ£€æŸ¥æˆæœ¬å’Œå®šä»·"
            elif anomaly_type == 'extreme_value':
                return "ç´§æ€¥æ ¸å®æ•°æ®æ¥æº"
            elif anomaly_type == 'declining_trend':
                return "åˆ¶å®šåº”å¯¹æªæ–½"
            else:
                return "éœ€è¦ç«‹å³å…³æ³¨"
        
        elif severity == 'medium':
            if anomaly_type in ['statistical_outlier', 'price_anomaly']:
                return "å»ºè®®æ ¸å®æ•°æ®"
            elif anomaly_type == 'high_profit_margin':
                return "ç¡®è®¤ä¸šåŠ¡åˆç†æ€§"
            else:
                return "å»ºè®®è¿›ä¸€æ­¥åˆ†æ"
        
        return "å¯é€‰æ‹©æ€§å¤„ç†"
    
    def _calculate_confidence(self, anomaly: Dict) -> float:
        """è®¡ç®—å¼‚å¸¸æ£€æµ‹ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        anomaly_type = anomaly.get('type', '')
        count = anomaly.get('count', 1)
        
        # åŸºç¡€ç½®ä¿¡åº¦
        base_confidence = 0.7
        
        # æ ¹æ®å¼‚å¸¸ç±»å‹è°ƒæ•´
        if anomaly_type in ['extreme_value', 'negative_profit']:
            base_confidence = 0.9  # é«˜ç½®ä¿¡åº¦
        elif anomaly_type in ['statistical_outlier', 'trend_break']:
            base_confidence = 0.8  # ä¸­é«˜ç½®ä¿¡åº¦
        elif anomaly_type in ['zero_value_anomaly', 'price_anomaly']:
            base_confidence = 0.75  # ä¸­ç­‰ç½®ä¿¡åº¦
        
        # æ ¹æ®å¼‚å¸¸æ•°é‡è°ƒæ•´
        if count >= 5:
            base_confidence += 0.1
        elif count >= 10:
            base_confidence += 0.15
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        return min(0.95, max(0.5, base_confidence))
    
    def get_anomaly_highlights(self, anomalies: List[Dict]) -> List[int]:
        """è·å–éœ€è¦åœ¨å¯è§†åŒ–ä¸­é«˜äº®çš„æ•°æ®ç‚¹ç´¢å¼•"""
        highlight_indices = []
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®å¼‚å¸¸ç±»å‹å’Œæ•°æ®ç‰¹å¾æ¥ç¡®å®šé«˜äº®ç‚¹
        for anomaly in anomalies:
            if anomaly.get("type") == "statistical_outlier" and "values" in anomaly:
                # å¯¹äºç»Ÿè®¡å¼‚å¸¸ï¼Œå¯ä»¥å°è¯•æ‰¾åˆ°å¯¹åº”çš„è¡Œç´¢å¼•
                # è¿™é‡Œç®€åŒ–ä¸ºè¿”å›å‰å‡ ä¸ªç´¢å¼•
                highlight_indices.extend(range(min(5, len(anomaly.get("values", [])))))
        
        return list(set(highlight_indices))  # å»é‡
        """è·å–éœ€è¦åœ¨å¯è§†åŒ–ä¸­é«˜äº®çš„æ•°æ®ç‚¹ç´¢å¼•"""
        highlight_indices = []
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®å¼‚å¸¸ç±»å‹å’Œæ•°æ®ç‰¹å¾æ¥ç¡®å®šé«˜äº®ç‚¹
        for anomaly in anomalies:
            if anomaly.get("type") == "statistical_outlier" and "values" in anomaly:
                # å¯¹äºç»Ÿè®¡å¼‚å¸¸ï¼Œå¯ä»¥å°è¯•æ‰¾åˆ°å¯¹åº”çš„è¡Œç´¢å¼•
                # è¿™é‡Œç®€åŒ–ä¸ºè¿”å›å‰å‡ ä¸ªç´¢å¼•
                highlight_indices.extend(range(min(5, len(anomaly.get("values", [])))))
        
        return list(set(highlight_indices))  # å»é‡

# å…¨å±€å®ä¾‹
anomaly_detector = AnomalyDetector()