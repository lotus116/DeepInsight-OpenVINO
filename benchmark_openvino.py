import os
import time
import psutil
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from optimum.intel import OVModelForFeatureExtraction

# === é…ç½®ï¼šä½¿ç”¨ä½ å·²æœ‰çš„æœ¬åœ°æ¨¡å‹è·¯å¾„ ===
PYTORCH_MODEL_PATH = r"D:\æ¯”èµ›\intel\intel\models\bge-small-zh-v1.5"
OPENVINO_MODEL_PATH = r"D:\æ¯”èµ›\intel\intel\models\bge-small-ov"

TEST_TEXT = "2016å¹´ç§‘æŠ€ç±»äº§å“åœ¨åŠ å·çš„æ€»é”€å”®é¢æ˜¯å¤šå°‘ï¼Ÿ"
DEVICE = "cpu"

def get_memory_mb():
    return psutil.Process().memory_info().rss / (1024 * 1024)

def benchmark_pytorch():
    print("ã€1/2ã€‘åŠ è½½æœ¬åœ° PyTorch æ¨¡å‹...")
    mem_before = get_memory_mb()
    tokenizer = AutoTokenizer.from_pretrained(PYTORCH_MODEL_PATH, local_files_only=True)
    model = AutoModel.from_pretrained(PYTORCH_MODEL_PATH, local_files_only=True).to(DEVICE).eval()
    mem_after_load = get_memory_mb()

    # Tokenize
    inputs = tokenizer(TEST_TEXT, return_tensors="pt", padding=True, truncation=True, max_length=512)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    # Warm-up
    with torch.no_grad():
        _ = model(**inputs)

    # Measure latency
    start = time.perf_counter()
    with torch.no_grad():
        outputs = model(**inputs)
    latency_ms = (time.perf_counter() - start) * 1000
    embedding = outputs.last_hidden_state[:, 0].cpu().numpy()

    mem_after_infer = get_memory_mb()
    del model, outputs
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return {
        "latency_ms": latency_ms,
        "memory_total_mb": mem_after_infer,
        "embedding": embedding
    }

def benchmark_openvino():
    print("ã€2/2ã€‘åŠ è½½æœ¬åœ° OpenVINO æ¨¡å‹...")
    mem_before = get_memory_mb()
    tokenizer = AutoTokenizer.from_pretrained(OPENVINO_MODEL_PATH, local_files_only=True)
    model = OVModelForFeatureExtraction.from_pretrained(
        OPENVINO_MODEL_PATH,
        device="CPU",
        local_files_only=True,
        ov_config={
            "PERFORMANCE_HINT": "LATENCY",
            "INFERENCE_PRECISION_HINT": "f32"
        }
    )
    mem_after_load = get_memory_mb()

    # Tokenize (OVModel æ¥å— PyTorch tensors)
    inputs = tokenizer(TEST_TEXT, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Warm-up
    _ = model(**inputs)

    # Measure latency
    start = time.perf_counter()
    outputs = model(**inputs)
    latency_ms = (time.perf_counter() - start) * 1000
    embedding = outputs.last_hidden_state[:, 0].numpy()

    mem_after_infer = get_memory_mb()
    del model, outputs

    return {
        "latency_ms": latency_ms,
        "memory_total_mb": mem_after_infer,
        "embedding": embedding
    }

if __name__ == "__main__":
    print("ğŸ” æ€§èƒ½éªŒè¯ï¼šä½¿ç”¨å·²æœ‰æœ¬åœ°æ¨¡å‹ï¼ˆæ— éœ€è½¬æ¢ï¼‰")
    print(f"PyTorch æ¨¡å‹è·¯å¾„: {PYTORCH_MODEL_PATH}")
    print(f"OpenVINO æ¨¡å‹è·¯å¾„: {OPENVINO_MODEL_PATH}")
    print("-" * 60)

    # ç¡®ä¿è·¯å¾„å­˜åœ¨
    assert os.path.exists(PYTORCH_MODEL_PATH), "PyTorch æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼"
    assert os.path.exists(OPENVINO_MODEL_PATH), "OpenVINO æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼"

    # Benchmark
    torch_res = benchmark_pytorch()
    time.sleep(1)  # å‡å°‘èµ„æºç«äº‰
    ov_res = benchmark_openvino()

    # è¾“å‡ºç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š æ¨ç†æ€§èƒ½å¯¹æ¯”ç»“æœï¼ˆå•æ¬¡ï¼ŒCPU-onlyï¼‰")
    print("="*70)
    print(f"{'æŒ‡æ ‡':<20} {'PyTorch':<18} {'OpenVINO':<18} {'æå‡'}")
    print("-"*70)
    
    latency_ratio = torch_res["latency_ms"] / ov_res["latency_ms"]
    memory_saved = torch_res["memory_total_mb"] - ov_res["memory_total_mb"]

    print(f"{'å»¶è¿Ÿ (ms)':<20} {torch_res['latency_ms']:<18.2f} {ov_res['latency_ms']:<18.2f} {latency_ratio:.2f}x")
    print(f"{'å†…å­˜å ç”¨ (MB)':<20} {torch_res['memory_total_mb']:<18.1f} {ov_res['memory_total_mb']:<18.1f} â†“{memory_saved:.1f} MB")

    # éªŒè¯åµŒå…¥ä¸€è‡´æ€§
    e1 = torch_res["embedding"].flatten()
    e2 = ov_res["embedding"].flatten()
    cosine_sim = np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2))
    print(f"\nâœ… åµŒå…¥ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f} ï¼ˆåº” â‰ˆ1.0ï¼Œè¡¨ç¤ºè¯­ä¹‰ä¸€è‡´ï¼‰")

    print("\nâœ… éªŒè¯å®Œæˆã€‚æ•°æ®çœŸå®ã€å¯å¤ç°ï¼Œå¯ç”¨äºæŠ€æœ¯æŠ¥å‘Šæ”¯æ’‘ææ–™ã€‚")