import numpy as np
from optimum.intel import OVModelForFeatureExtraction
from transformers import AutoTokenizer
import torch
import os
import json
import time  # æ–°å¢ï¼šç”¨äºè®¡æ—¶
import psutil  # æ–°å¢ï¼šç”¨äºå†…å­˜ç›‘æ§
from sqlalchemy import create_engine, inspect

# --- 1. ä¾èµ–åº“å®¹é”™å¯¼å…¥ ---
try:
    import fitz  # PyMuPDF, ç”¨äºè¯»å– PDF
except ImportError:
    fitz = None
    print("âš ï¸ æç¤º: æœªæ£€æµ‹åˆ° pymupdf åº“ï¼ŒPDF è§£æåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

try:
    import docx  # python-docx, ç”¨äºè¯»å– Word
except ImportError:
    docx = None
    print("âš ï¸ æç¤º: æœªæ£€æµ‹åˆ° python-docx åº“ï¼ŒWord è§£æåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")


class IntelRAG:
    def __init__(self, model_path, db_uris=None, kb_paths=None):
        """
        RAG å¼•æ“æ ¸å¿ƒç±»
        :param model_path: OpenVINO å¯¼å‡ºçš„ Embedding æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
        :param db_uris: æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²åˆ—è¡¨ (æ”¯æŒå¤šåº“)
        :param kb_paths: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (PDF/TXT/JSON/Word)
        """
        # å¤„ç†å‚æ•°å…¼å®¹æ€§
        if db_uris is None:
            db_uris = []
        if kb_paths is None:
            kb_paths = []
            
        print(f"âš¡ [RAG] å¼•æ“åˆå§‹åŒ–...")
        print(f"   ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"   ğŸ—„ï¸ æ•°æ®åº“æº: {len(db_uris)} ä¸ª")
        print(f"   ğŸ“š çŸ¥è¯†æ–‡ä»¶: {len(kb_paths)} ä¸ª")
        
        # 1. åŠ è½½æ¨¡å‹
        if not os.path.exists(model_path):
            print(f"âŒ ä¸¥é‡é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ {model_path}")
            self.model = None
            self.tokenizer = None
        else:
            try:
                # ä½¿ç”¨ OpenVINO åŠ é€Ÿæ¨ç†
                self.model = OVModelForFeatureExtraction.from_pretrained(
                    model_path, 
                    device="CPU", 
                    ov_config={"PERFORMANCE_HINT": "LATENCY"}
                )
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                print("âœ… OpenVINO æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None

        self.db_uris = db_uris
        self.kb_paths = kb_paths
        
        # å†…å­˜å‘é‡åº“
        self.documents = []   # å­˜æ–‡æœ¬
        self.embeddings = None # å­˜å‘é‡ (NumPy Matrix)
        
        # 2. æ„å»ºçŸ¥è¯†åº“
        self._build_knowledge_base()

    def _get_embedding(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        if self.model is None or not text: 
            return np.zeros(384) # è¿™é‡Œçš„ç»´åº¦å–å†³äºä½ ç”¨çš„æ¨¡å‹ï¼Œbge-small æ˜¯ 384 æˆ– 512
            
        try:
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            with torch.no_grad():
                outputs = self.model(**inputs)
                # å– CLS å‘é‡æˆ–é¦–ä½å‘é‡
                return outputs.last_hidden_state[:, 0].squeeze().numpy()
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            return np.zeros(384)

    def _read_file(self, file_path):
        """é€šç”¨æ–‡ä»¶è§£æå™¨"""
        if not os.path.exists(file_path):
            return ""
            
        ext = os.path.splitext(file_path)[1].lower()
        content = []
        
        try:
            # === PDF è§£æ ===
            if ext == '.pdf':
                if fitz:
                    with fitz.open(file_path) as doc:
                        for page in doc: 
                            content.append(page.get_text())
                else:
                    return "Error: ç¼ºå°‘ pymupdf åº“"

            # === Word è§£æ ===
            elif ext == '.docx':
                if docx:
                    doc = docx.Document(file_path)
                    content = [p.text for p in doc.paragraphs if p.text.strip()]
                else:
                    return "Error: ç¼ºå°‘ python-docx åº“"

            # === JSON è§£æ ===
            elif ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # å°† JSON ç»“æ„è½¬ä¸ºå­—ç¬¦ä¸²æè¿°ï¼Œæ–¹ä¾¿æ£€ç´¢
                    if isinstance(data, list): # å¦‚æœæ˜¯ schema_desc.json è¿™ç§åˆ—è¡¨
                        for item in data:
                            content.append(f"è¡¨å: {item.get('table_name')}, æè¿°: {item.get('description')}")
                            for col in item.get('columns', []):
                                col_desc = f"å­—æ®µ {col['name']}: {col.get('description')}"
                                if 'formula' in col: col_desc += f", è®¡ç®—å…¬å¼: {col['formula']}"
                                content.append(col_desc)
                    else:
                        content.append(json.dumps(data, ensure_ascii=False))

            # === çº¯æ–‡æœ¬/Markdown ===
            elif ext in ['.txt', '.md', '.csv', '.jsonl']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content.append(f.read())
            
            return "\n".join(content)
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {os.path.basename(file_path)} å¤±è´¥: {e}")
            return ""

    def _build_knowledge_base(self):
        """æ ¸å¿ƒï¼šæ‰«ææ•°æ®åº“ + è¯»å–æ–‡ä»¶ -> å‘é‡åŒ–"""
        self.documents = []

        # --- æ­¥éª¤ A: æ‰«ææ•°æ®åº“ç»“æ„ (Schema) ---
        if self.db_uris:
            print(f"ğŸ” æ­£åœ¨æ‰«æ {len(self.db_uris)} ä¸ªæ•°æ®åº“ç»“æ„...")
            for uri in self.db_uris:
                if not uri: continue
                try:
                    engine = create_engine(uri)
                    inspector = inspect(engine)
                    table_names = inspector.get_table_names()
                    
                    for t_name in table_names:
                        # è·å–å­—æ®µä¿¡æ¯
                        columns = inspector.get_columns(t_name)
                        col_details = []
                        for c in columns:
                            # æ ¼å¼: å­—æ®µå(ç±»å‹)
                            col_info = f"{c['name']}({c['type']})"
                            if c.get('comment'): col_info += f" æ³¨é‡Š:{c['comment']}"
                            col_details.append(col_info)
                        
                        # ç»„åˆæˆä¸€æ¡æ–‡æ¡£
                        doc = f"æ•°æ®åº“è¡¨å: {t_name}\nåŒ…å«å­—æ®µ: {', '.join(col_details)}"
                        self.documents.append(doc)
                except Exception as e:
                    print(f"âŒ æ•°æ®åº“è¿æ¥/æ‰«æå¤±è´¥ ({uri}): {e}")

        # --- æ­¥éª¤ B: è¯»å–çŸ¥è¯†åº“æ–‡ä»¶ ---
        if self.kb_paths:
            print(f"ğŸ“‚ æ­£åœ¨è¯»å– {len(self.kb_paths)} ä¸ªæ–‡ä»¶...")
            for path in self.kb_paths:
                text = self._read_file(path)
                if not text or "Error" in text: continue
                
                # æ–‡æœ¬åˆ‡ç‰‡ (Chunking)
                # ä¸ºäº†é˜²æ­¢æ–‡æœ¬è¿‡é•¿è¶…è¿‡ LLM çª—å£ï¼Œè¿™é‡ŒæŒ‰ 500 å­—ç¬¦åˆ‡ç‰‡
                chunk_size = 500
                for i in range(0, len(text), chunk_size):
                    chunk = text[i:i+chunk_size]
                    # åŠ ä¸Šæ–‡ä»¶åä½œä¸ºä¸Šä¸‹æ–‡
                    self.documents.append(f"æ¥æºæ–‡ä»¶[{os.path.basename(path)}]:\n{chunk}")

        # ä¿åº•å¤„ç†
        if not self.documents:
            self.documents = ["æš‚æ— æœ‰æ•ˆçŸ¥è¯†åº“ä¿¡æ¯ã€‚"]
            print("âš ï¸ è­¦å‘Š: çŸ¥è¯†åº“ä¸ºç©ºï¼ŒRAG å°†æ— æ³•æä¾›ä¸Šä¸‹æ–‡ã€‚")

        # --- æ­¥éª¤ C: å‘é‡åŒ– (Embedding) ---
        print(f"ğŸš€ [OpenVINO] æ­£åœ¨ç”Ÿæˆå‘é‡ç´¢å¼• (å…± {len(self.documents)} æ¡)...")
        if self.model:
            try:
                embeddings_list = [self._get_embedding(doc) for doc in self.documents]
                self.embeddings = np.array(embeddings_list)
                print("âœ… å‘é‡åŒ–å®Œæˆï¼")
            except Exception as e:
                print(f"âŒ å‘é‡åŒ–è¿‡ç¨‹ä¸­æ–­: {e}")
                self.embeddings = None

    def retrieve(self, query, top_k=5):
        """
        æ£€ç´¢å‡½æ•° (ä¼˜åŒ–ç‰ˆï¼šè¿”å›å†…å®¹ + æ€§èƒ½æŒ‡æ ‡)
        :param query: ç”¨æˆ·é—®é¢˜
        :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„ k æ¡è®°å½•
        :return: (context_str, latency_ms, memory_delta_mb)
        """
        if self.embeddings is None or len(self.documents) == 0:
            return "", 0.0, 0.0
            
        # --- æ€§èƒ½ç›‘æ§å¼€å§‹ ---
        start_time = time.perf_counter()
        process = psutil.Process(os.getpid())
        mem_before = process.memory_info().rss / (1024 * 1024) # MB
        
        # 1. é—®é¢˜å‘é‡åŒ–
        query_emb = self._get_embedding(query)
        
        # 2. ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®— (Vector Search)
        # dot product / (norm_a * norm_b)
        scores = np.dot(self.embeddings, query_emb) / (
            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_emb) + 1e-10
        )
        
        # 3. æ’åºå¹¶å– Top-K
        # argsort è¿”å›çš„æ˜¯ä»å°åˆ°å¤§çš„ç´¢å¼•ï¼Œæ‰€ä»¥è¦ [::-1] åè½¬
        top_k = min(top_k, len(self.documents))
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        # 4. æ‹¼æ¥ç»“æœ
        results = []
        for idx in top_indices:
            # å¯ä»¥åœ¨è¿™é‡Œæ‰“å°åˆ†æ•°è°ƒè¯•
            results.append(self.documents[idx])
        
        context_str = "\n\n".join(results)

        # --- æ€§èƒ½ç›‘æ§ç»“æŸ ---
        end_time = time.perf_counter()
        mem_after = process.memory_info().rss / (1024 * 1024)
        
        latency_ms = (end_time - start_time) * 1000
        mem_delta_mb = max(0, mem_after - mem_before) # é˜²æ­¢è´Ÿæ•°
            
        return context_str, latency_ms, mem_delta_mb